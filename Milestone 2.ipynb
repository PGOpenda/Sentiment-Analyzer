{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f78ec4",
   "metadata": {},
   "source": [
    "# <em><u>Sentiment Analyzer - Milestone 2</u></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08fe290",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf7782c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\piuso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\piuso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\piuso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\piuso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from IPython.display import display\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the NLTK tokenizer for Kiswahili\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb756fde",
   "metadata": {},
   "source": [
    "## Load our clean data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d7c8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bahati mbaya fadhila yoyote kazi utengenezaji ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huwa kuliko msemaji mwingine yeyote ukubwa huu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iligundua rahisi kutengeneza kutumia bidhaa</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ipurkated hii ajili msimamizi gari haifaulu</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sinema nzuri kwelikweli upendo usio masharti</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  bahati mbaya fadhila yoyote kazi utengenezaji ...  negative\n",
       "1  huwa kuliko msemaji mwingine yeyote ukubwa huu...  positive\n",
       "2        iligundua rahisi kutengeneza kutumia bidhaa  positive\n",
       "3        ipurkated hii ajili msimamizi gari haifaulu  negative\n",
       "4       sinema nzuri kwelikweli upendo usio masharti  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe3001c",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier using scikit-learn library\n",
    "\n",
    "1. Here we train a model with our cleaned dataset\n",
    "2. Then we get the accuracy of the model using a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9014b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.80%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75       382\n",
      "    positive       0.76      0.77      0.77       403\n",
      "\n",
      "    accuracy                           0.76       785\n",
      "   macro avg       0.76      0.76      0.76       785\n",
      "weighted avg       0.76      0.76      0.76       785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data = df.sample(frac=0.8, random_state=42)\n",
    "test_data = df.drop(train_data.index)\n",
    "\n",
    "# Extract the features and labels from the training and testing data\n",
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform(train_data['text'].values.astype('U'))\n",
    "test_features = vectorizer.transform(test_data['text'].values.astype('U'))\n",
    "train_labels = train_data['labels']\n",
    "test_labels = test_data['labels']\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the testing data\n",
    "predictions = classifier.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "report = classification_report(test_labels, predictions)\n",
    "\n",
    "# Print the evaluation results\n",
    "print('Accuracy: {:.2f}%'.format(accuracy*100))\n",
    "print('Classification Report:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06d366",
   "metadata": {},
   "source": [
    "## Save the trained model and vectorizer as joblib file\n",
    "\n",
    "1. We will use this joblib files in making a web interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f89825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swahili_naive_bayes.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(classifier, 'swahili_naive_bayes.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4456ff03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swahili_vectorizer.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer, 'swahili_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad0103",
   "metadata": {},
   "source": [
    "## Classify new text data\n",
    "\n",
    "### Function that allows us to preprocess new text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb012bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TULIENDA HUKO LAKINI HUYO MAMA ALIKUWA AMEENDA NA KILA KITU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tulienda huko huyo mama ameenda kitu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    swahili_stopwords = stopwords.words('swahili')\n",
    "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)# Remove special characters\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    tokens = word_tokenize(text) # Tokenize the text\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in swahili_stopwords] # Remove the stopwords   \n",
    "    filtered_text = ' '.join(filtered_tokens) # Join the filtered tokens back into a string\n",
    "    return filtered_text\n",
    "\n",
    "preprocess(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d50519",
   "metadata": {},
   "source": [
    "### Asking a user for new text data with an input box and displaying the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b78894a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your swahili text: Sitaki ujinga\n",
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "# get teh user input\n",
    "text = input(\"Enter your swahili text: \")\n",
    "\n",
    "# Preprocess the text wiht our funciton\n",
    "new_text = [preprocess(text)]\n",
    "\n",
    "# Extract the features and try and predict the label forom our model\n",
    "new_features = vectorizer.transform(new_text)\n",
    "predicted_label = classifier.predict(new_features)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29fe2d",
   "metadata": {},
   "source": [
    "## Deployment and hosting\n",
    "\n",
    "1. I have created a flask application that consists of html, css, python and javascript files. The folder for the application is Flask_App which can be found in the Github repository \n",
    "2. We use the joblib files for the model and vectorizer to predict sentiment of user input on the site. \n",
    "3. The site is hosted on <a href=\"https://www.pythonanywhere.com/\">pythonanyhwere.com</a> \n",
    "4. The link to my site is <a href=\"https://openda.pythonanywhere.com/\">openda.pythonanywhere.com</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a575fd54",
   "metadata": {},
   "source": [
    "# <em><u>THE END</u></em>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
