{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd10ef3",
   "metadata": {},
   "source": [
    "# <em><u>Sentiment Analyzer</u></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c39b1",
   "metadata": {},
   "source": [
    "## Import the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffe709f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\piuso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\piuso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\piuso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\piuso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import joblib\n",
    "import ipywidgets as widgets\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialize the NLTK tokenizer for Kiswahili\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea921685",
   "metadata": {},
   "source": [
    "## Load our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9fe3990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>team 2019merimera alikuwa takataka</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sijafurahishwa</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kubuni dosari</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bila kusema nilipoteza pesa zangu</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sema kupoteza pesa na wakati</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ubunifu ni isiyo ya kawaida sana kwani kipande...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>akili yako imeoza</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aki si maisha ni magumu</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>enyewe safaricom ni wezi</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mtandao duni hata line yao niliweka nyuma ya s...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0                 team 2019merimera alikuwa takataka  negative\n",
       "1                                     sijafurahishwa  negative\n",
       "2                                      kubuni dosari  negative\n",
       "3                  bila kusema nilipoteza pesa zangu  negative\n",
       "4                       sema kupoteza pesa na wakati  negative\n",
       "5  ubunifu ni isiyo ya kawaida sana kwani kipande...  negative\n",
       "6                                  akili yako imeoza  negative\n",
       "7                            aki si maisha ni magumu  negative\n",
       "8                           enyewe safaricom ni wezi  negative\n",
       "9  mtandao duni hata line yao niliweka nyuma ya s...  negative"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('swahili.csv')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f069eb7",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b13dc",
   "metadata": {},
   "source": [
    "1. Remove URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bec17daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3920    Nafikiri chakula chapasa kuwa na ladha na umbi...\n",
      "191     mojawapo ya mambo yanayokatisha tamaa zaidi ni...\n",
      "109     lakini wakati mtu anajitahidi kwa ukuu na usha...\n",
      "3367    Hata hivyo mkahawa huu una kifungua - kinywa k...\n",
      "447                                        krismasi njema\n",
      "2918                 lilionekana kama hadithi nzuri ajabu\n",
      "2090    Hakuna mtu atambulishaye herufi hizi kwa sabab...\n",
      "793     sinema hii inasawazishwa vizuri na vichekesho ...\n",
      "176     tukiacha ubaguzi wa rangi wacha tuangalie upun...\n",
      "313                                         haiweki shati\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Remove the urls\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "df['text'] = df['text'].apply(remove_urls)\n",
    "\n",
    "#print out a sample\n",
    "print(df['text'].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df08e6f9",
   "metadata": {},
   "source": [
    "2. Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7d65f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2786    Tokeo ni mafanikio ya ujenzi wa nguvu za umeme...\n",
      "1824    vifaa hivyo viwili vilikuwa rahisi sana hivi k...\n",
      "619     sinema hii ni maoni mazuri na ya kuchekesha ju...\n",
      "1581    Hatimaye baada ya mara tatu au nne masika ya n...\n",
      "3536    Nilichunguza mahali hapa miaka kadhaa iliyopit...\n",
      "1976    Mbali na kuwa na moja ya nyimbo zenye kupendez...\n",
      "3440    Chakula ni kizuri sana kwa chakula chako cha k...\n",
      "2647          Ni kama sinema nzuri au nzuri isiyotabirika\n",
      "1186    mwezi mmoja tu ulikuwa na kazi yake lakini bil...\n",
      "2040    mandhari za majirani wa ohsovierl zilizokomaa ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Remove special characters\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' # keep only alphanumeric and whitespace characters\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(remove_special_characters)\n",
    "\n",
    "#print out a sample\n",
    "print(df['text'].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9aa646",
   "metadata": {},
   "source": [
    "3. Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1db807a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110     jambo moja zaidi naweza kuvumilia usahihi wa k...\n",
      "470                                         shukrani sana\n",
      "1150       hakika hii bila ya shaka ni haki ya kupambanua\n",
      "1877                     zikapangwa vizuri pamoja na hilo\n",
      "1622                   haikufanyi uonekane mwenye ubaridi\n",
      "696     nadhani nilipenda maelezo ya dysfunction yake ...\n",
      "3207                                i alingoja na kungoja\n",
      "1511                                betri inashika vizuri\n",
      "2576    ni ajabu kwamba hadithi hizo zimefumwa pamoja ...\n",
      "2181                       lililokuwa jambo la kuchekesha\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#convert to lower case\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "#print out a sample\n",
    "print(df['text'].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c258e",
   "metadata": {},
   "source": [
    "4. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e33a871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3379       tatu mwisho chakula mchana hapa zimekuwa mbaya\n",
      "2499    mbali risasi mbaya filamu ina mizigo mingi vit...\n",
      "3921                                  hamu kula ilitoweka\n",
      "1253                                         kiruu amazon\n",
      "3043                 namna ugumu kuharibu nyama walifanya\n",
      "304                                             neno aibu\n",
      "1904                                      si buku kutosha\n",
      "1209               niliipata sababu ndogo yenye kupendeza\n",
      "1895    imeiunganisha kujua si kitu kilichotengenezwa ...\n",
      "933               kusema sababu natumia vibaya pesa zangu\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Download the stopwords for Kiswahili language\n",
    "swahili_stopwords = stopwords.words('swahili')\n",
    "\n",
    "# Define a function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove the stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in swahili_stopwords]\n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# Apply the remove_stopwords function to the 'text' column of the DataFrame\n",
    "df['text'] = df['text'].apply(remove_stopwords)\n",
    "\n",
    "print(df['text'].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef09fb",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier using scikit-learn library\n",
    "\n",
    "1. Here we train a model with our cleaned dataset\n",
    "2. Then we get the accuracy of the model using a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ddc20f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.05%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.70      0.74       387\n",
      "    positive       0.74      0.82      0.78       398\n",
      "\n",
      "    accuracy                           0.76       785\n",
      "   macro avg       0.76      0.76      0.76       785\n",
      "weighted avg       0.76      0.76      0.76       785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing sets\n",
    "train_data = df.sample(frac=0.8, random_state=42)\n",
    "test_data = df.drop(train_data.index)\n",
    "\n",
    "# extract the features and labels from the training and testing data\n",
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform(train_data['text'])\n",
    "test_features = vectorizer.transform(test_data['text'])\n",
    "train_labels = train_data['labels']\n",
    "test_labels = test_data['labels']\n",
    "\n",
    "# train the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# evaluate the classifier on the testing data\n",
    "predictions = classifier.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "report = classification_report(test_labels, predictions)\n",
    "\n",
    "# print the evaluation results\n",
    "print('Accuracy: {:.2f}%'.format(accuracy*100))\n",
    "print('Classification Report:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b7dbae",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee743a",
   "metadata": {},
   "source": [
    "Save the trained model to my local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0563517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swahili_naive_bayes_model.joblib']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(classifier, 'swahili_naive_bayes_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e9632",
   "metadata": {},
   "source": [
    "#### Classify new text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7597cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your swahili text: Mimi napenda huyo mwalimu\n",
      "['positive']\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter your swahili text: \")\n",
    "new_text = [text]\n",
    "new_features = vectorizer.transform(new_text)\n",
    "predicted_label = classifier.predict(new_features)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f814a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
